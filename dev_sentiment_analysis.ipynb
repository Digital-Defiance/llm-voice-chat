{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ModelFactory\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "model_factory = ModelFactory(\n",
    "    coordinates = 6*8,\n",
    "    words = 70,\n",
    "    tokens=50258 + 1,\n",
    "    number_of_blocks = 1,\n",
    "    number_of_heads = 6,\n",
    "    bias = 0,\n",
    "    attention = \"metric\"# \"scaled_dot_product\", # or \"metric\"\n",
    ")\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model_factory.create_model(kind=\"encoder\")\n",
    "        del self.model[-1]\n",
    "        del self.model[-1]\n",
    "\n",
    " \n",
    "        self.layer_norm = nn.LayerNorm(model_factory.coordinates)\n",
    "        self.projection = nn.Sequential(\n",
    "          nn.LayerNorm(model_factory.coordinates),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(model_factory.coordinates, model_factory.coordinates // 2),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(model_factory.coordinates // 2, model_factory.coordinates // 2),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(model_factory.coordinates // 2, 1),\n",
    "        )\n",
    "          \n",
    "      \n",
    "        self.classification_head = nn.Sequential(\n",
    "          nn.Linear(model_factory.words, model_factory.words),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(model_factory.words, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence_bw: Tensor) -> Tensor:\n",
    "        sequence_bwc = self.model(sequence_bw)\n",
    "        sequence_bwc = self.layer_norm(sequence_bwc)\n",
    "        sequence_bw1 = self.projection(sequence_bwc)\n",
    "        sequence_bw = sequence_bw1.squeeze(-1)\n",
    "        sequence_bv = self.classification_head(sequence_bw)\n",
    "        return sequence_bv\n",
    "\n",
    "with open(\"stanfordSentimentTreebank.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1):   0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1): 100%|██████████| 41/41 [00:07<00:00,  5.80it/s, avg loss=0.00806, lr=0.00161]\n",
      "(2): 100%|██████████| 41/41 [00:06<00:00,  6.02it/s, avg loss=0.00804, lr=0.00161]\n",
      "(3): 100%|██████████| 41/41 [00:07<00:00,  5.82it/s, avg loss=0.00799, lr=0.00161]\n",
      "(4): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.00773, lr=0.00161]\n",
      "(5): 100%|██████████| 41/41 [00:07<00:00,  5.55it/s, avg loss=0.00728, lr=0.00161]\n",
      "(6): 100%|██████████| 41/41 [00:06<00:00,  5.94it/s, avg loss=0.00676, lr=0.00161]\n",
      "(7): 100%|██████████| 41/41 [00:06<00:00,  6.08it/s, avg loss=0.006, lr=0.00161]  \n",
      "(8): 100%|██████████| 41/41 [00:06<00:00,  6.13it/s, avg loss=0.00532, lr=0.00161]\n",
      "(9): 100%|██████████| 41/41 [00:06<00:00,  5.99it/s, avg loss=0.00475, lr=0.00161]\n",
      "(10): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.00424, lr=0.00161]\n",
      "(11): 100%|██████████| 41/41 [00:06<00:00,  6.14it/s, avg loss=0.00371, lr=0.00161]\n",
      "(12): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.00339, lr=0.00161]\n",
      "(13): 100%|██████████| 41/41 [00:06<00:00,  6.05it/s, avg loss=0.00301, lr=0.00161]\n",
      "(14): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=0.0027, lr=0.00161] \n",
      "(15): 100%|██████████| 41/41 [00:06<00:00,  6.13it/s, avg loss=0.00244, lr=0.00161]\n",
      "(16): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=0.00223, lr=0.00161]\n",
      "(17): 100%|██████████| 41/41 [00:06<00:00,  6.15it/s, avg loss=0.00211, lr=0.00161]\n",
      "(18): 100%|██████████| 41/41 [00:06<00:00,  6.06it/s, avg loss=0.00184, lr=0.00161]\n",
      "(19): 100%|██████████| 41/41 [00:06<00:00,  6.15it/s, avg loss=0.00174, lr=0.00161]\n",
      "(20): 100%|██████████| 41/41 [00:06<00:00,  5.94it/s, avg loss=0.00163, lr=0.00161]\n",
      "(21): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.00147, lr=0.00161]\n",
      "(22): 100%|██████████| 41/41 [00:06<00:00,  6.16it/s, avg loss=0.00153, lr=0.00161]\n",
      "(23): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.00148, lr=0.00161]\n",
      "(24): 100%|██████████| 41/41 [00:06<00:00,  6.11it/s, avg loss=0.00138, lr=0.00161]\n",
      "(25): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=0.00133, lr=0.00161]\n",
      "(26): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=0.00113, lr=0.00161]\n",
      "(27): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.00109, lr=0.00161]\n",
      "(28): 100%|██████████| 41/41 [00:06<00:00,  6.04it/s, avg loss=0.00102, lr=0.00161]\n",
      "(29): 100%|██████████| 41/41 [00:06<00:00,  6.04it/s, avg loss=0.000937, lr=0.00161]\n",
      "(30): 100%|██████████| 41/41 [00:06<00:00,  5.91it/s, avg loss=0.000826, lr=0.00161]\n",
      "(31): 100%|██████████| 41/41 [00:06<00:00,  6.04it/s, avg loss=0.000815, lr=0.00161]\n",
      "(32): 100%|██████████| 41/41 [00:06<00:00,  6.11it/s, avg loss=0.00121, lr=0.00161]\n",
      "(33): 100%|██████████| 41/41 [00:06<00:00,  6.15it/s, avg loss=0.0012, lr=0.00161] \n",
      "(34): 100%|██████████| 41/41 [00:06<00:00,  6.01it/s, avg loss=0.000919, lr=0.00161]\n",
      "(35): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.000744, lr=0.00161]\n",
      "(36): 100%|██████████| 41/41 [00:06<00:00,  6.16it/s, avg loss=0.000693, lr=0.00161]\n",
      "(37): 100%|██████████| 41/41 [00:06<00:00,  5.99it/s, avg loss=0.000734, lr=0.00161]\n",
      "(38): 100%|██████████| 41/41 [00:06<00:00,  5.97it/s, avg loss=0.000691, lr=0.00161]\n",
      "(39): 100%|██████████| 41/41 [00:06<00:00,  6.03it/s, avg loss=0.000619, lr=0.00161]\n",
      "(40): 100%|██████████| 41/41 [00:06<00:00,  5.95it/s, avg loss=0.000575, lr=0.00161]\n",
      "(41): 100%|██████████| 41/41 [00:06<00:00,  6.01it/s, avg loss=0.000492, lr=0.00161]\n",
      "(42): 100%|██████████| 41/41 [00:06<00:00,  6.16it/s, avg loss=0.000466, lr=0.00161]\n",
      "(43): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.000474, lr=0.00161]\n",
      "(44): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.000914, lr=0.00161]\n",
      "(45): 100%|██████████| 41/41 [00:06<00:00,  6.13it/s, avg loss=0.00074, lr=0.00161] \n",
      "(46): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.000599, lr=0.00161]\n",
      "(47): 100%|██████████| 41/41 [00:06<00:00,  5.99it/s, avg loss=0.000521, lr=0.00161]\n",
      "(48): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.000521, lr=0.00161]\n",
      "(49): 100%|██████████| 41/41 [00:06<00:00,  6.04it/s, avg loss=0.000463, lr=0.00161]\n",
      "(50): 100%|██████████| 41/41 [00:06<00:00,  6.02it/s, avg loss=0.000383, lr=0.00161]\n",
      "(51): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.000332, lr=0.00161]\n",
      "(52): 100%|██████████| 41/41 [00:06<00:00,  6.04it/s, avg loss=0.000284, lr=0.00161]\n",
      "(53): 100%|██████████| 41/41 [00:06<00:00,  6.02it/s, avg loss=0.000362, lr=0.00161]\n",
      "(54): 100%|██████████| 41/41 [00:06<00:00,  6.10it/s, avg loss=0.000506, lr=0.00161]\n",
      "(55): 100%|██████████| 41/41 [00:06<00:00,  6.03it/s, avg loss=0.000425, lr=0.00161]\n",
      "(56): 100%|██████████| 41/41 [00:06<00:00,  5.90it/s, avg loss=0.000574, lr=0.00161]\n",
      "(57): 100%|██████████| 41/41 [00:06<00:00,  6.16it/s, avg loss=0.000664, lr=0.00161]\n",
      "(58): 100%|██████████| 41/41 [00:06<00:00,  6.06it/s, avg loss=0.000471, lr=0.00161]\n",
      "(59): 100%|██████████| 41/41 [00:06<00:00,  5.96it/s, avg loss=0.000316, lr=0.00161]\n",
      "(60): 100%|██████████| 41/41 [00:06<00:00,  6.15it/s, avg loss=0.000227, lr=0.00161]\n",
      "(61): 100%|██████████| 41/41 [00:06<00:00,  6.13it/s, avg loss=0.000216, lr=0.00161]\n",
      "(62): 100%|██████████| 41/41 [00:06<00:00,  5.98it/s, avg loss=0.000216, lr=0.00161]\n",
      "(63): 100%|██████████| 41/41 [00:06<00:00,  6.11it/s, avg loss=0.000385, lr=0.00161]\n",
      "(64): 100%|██████████| 41/41 [00:06<00:00,  6.13it/s, avg loss=0.000595, lr=0.00161]\n",
      "(65): 100%|██████████| 41/41 [00:06<00:00,  5.94it/s, avg loss=0.000663, lr=0.00161]\n",
      "(66): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=0.000477, lr=0.00161]\n",
      "(67): 100%|██████████| 41/41 [00:06<00:00,  5.87it/s, avg loss=0.000318, lr=0.00161]\n",
      "(68): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.000181, lr=0.00161]\n",
      "(69): 100%|██████████| 41/41 [00:06<00:00,  6.08it/s, avg loss=0.000211, lr=0.00161]\n",
      "(70): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.000165, lr=0.00161]\n",
      "(71): 100%|██████████| 41/41 [00:06<00:00,  6.01it/s, avg loss=0.000146, lr=0.00161]\n",
      "(72): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.0004, lr=0.00161]  \n",
      "(73): 100%|██████████| 41/41 [00:06<00:00,  6.12it/s, avg loss=0.000744, lr=0.00161]\n",
      "(74): 100%|██████████| 41/41 [00:06<00:00,  6.04it/s, avg loss=0.000469, lr=0.00161]\n",
      "(75): 100%|██████████| 41/41 [00:06<00:00,  6.03it/s, avg loss=0.000348, lr=0.00161]\n",
      "(76): 100%|██████████| 41/41 [00:06<00:00,  6.10it/s, avg loss=0.000225, lr=0.00161]\n",
      "(77): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=0.000164, lr=0.00161]\n",
      "(78): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=0.000559, lr=0.00161]\n",
      "(79): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=0.000304, lr=0.00161]\n",
      "(80): 100%|██████████| 41/41 [00:06<00:00,  6.06it/s, avg loss=0.000279, lr=0.00161]\n",
      "(81): 100%|██████████| 41/41 [00:06<00:00,  6.10it/s, avg loss=0.000247, lr=0.00161]\n",
      "(82): 100%|██████████| 41/41 [00:06<00:00,  6.13it/s, avg loss=0.000159, lr=0.00161]\n",
      "(83): 100%|██████████| 41/41 [00:06<00:00,  6.03it/s, avg loss=9.77e-5, lr=0.00161] \n",
      "(84): 100%|██████████| 41/41 [00:06<00:00,  6.06it/s, avg loss=7.4e-5, lr=0.00161]  \n",
      "(85): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=5.44e-5, lr=0.00161]\n",
      "(86): 100%|██████████| 41/41 [00:06<00:00,  6.11it/s, avg loss=4.86e-5, lr=0.00161]\n",
      "(87): 100%|██████████| 41/41 [00:06<00:00,  6.04it/s, avg loss=4.07e-5, lr=0.00161]\n",
      "(88): 100%|██████████| 41/41 [00:06<00:00,  5.98it/s, avg loss=3.68e-5, lr=0.00161]\n",
      "(89): 100%|██████████| 41/41 [00:06<00:00,  6.03it/s, avg loss=3.14e-5, lr=0.00161]\n",
      "(90): 100%|██████████| 41/41 [00:06<00:00,  6.03it/s, avg loss=2.74e-5, lr=0.00161]\n",
      "(91): 100%|██████████| 41/41 [00:06<00:00,  6.01it/s, avg loss=2.36e-5, lr=0.00161]\n",
      "(92): 100%|██████████| 41/41 [00:06<00:00,  6.10it/s, avg loss=2.03e-5, lr=0.00161]\n",
      "(93): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=1.73e-5, lr=0.00161]\n",
      "(94): 100%|██████████| 41/41 [00:06<00:00,  6.06it/s, avg loss=1.52e-5, lr=0.00161]\n",
      "(95): 100%|██████████| 41/41 [00:06<00:00,  6.07it/s, avg loss=1.3e-5, lr=0.00161] \n",
      "(96): 100%|██████████| 41/41 [00:06<00:00,  6.09it/s, avg loss=9.96e-6, lr=0.00161]\n",
      "(97): 100%|██████████| 41/41 [00:06<00:00,  5.97it/s, avg loss=9.21e-6, lr=0.00161]\n",
      "(98): 100%|██████████| 41/41 [00:06<00:00,  6.08it/s, avg loss=9.85e-6, lr=0.00161]\n",
      "(99): 100%|██████████| 41/41 [00:06<00:00,  5.96it/s, avg loss=9.27e-6, lr=0.00161]\n",
      "(100): 100%|██████████| 41/41 [00:07<00:00,  5.83it/s, avg loss=8.64e-6, lr=0.00161]\n"
     ]
    }
   ],
   "source": [
    "print(\"here\")\n",
    "DEVICE = \"cpu\" # \"cuda\"\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "gpt2_encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SentimentModel().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "class_weights = torch.tensor([1.1, 1, 0.95, 1, 1.1], device=DEVICE)\n",
    "loss_function = nn.CrossEntropyLoss() # weight=class_weights\n",
    "batch_size = 200\n",
    "sentences, sentiments = dataset['train']\n",
    "size = len(sentences)\n",
    "\n",
    "warmup_steps = 8\n",
    "get_lr = lambda step: min(step ** -0.5, step * warmup_steps ** -1.5) * model_factory.coordinates ** -0.5\n",
    "EPOCHS = 100\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    \n",
    "    training_loss_cumul = 0\n",
    "    pb = tqdm(\n",
    "          range(0, size, batch_size),\n",
    "          desc=f\"({epoch})\",\n",
    "          leave=True,\n",
    "    )\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    indices = np.arange(len(sentences))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Use the shuffled indices to randomize sentences and sentiments\n",
    "    sentences = sentences[indices]\n",
    "    sentiments = sentiments[indices]\n",
    "\n",
    "    for i in pb:\n",
    "        lr = get_lr(i + 1)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        sentence_bw = torch.tensor(sentences[i:i+batch_size]).to(DEVICE)\n",
    "        sentiment_b = torch.tensor(sentiments[i:i+batch_size]).to(DEVICE)\n",
    "  \n",
    "        optimizer.zero_grad()\n",
    "        pred_logits_bv = model(sentence_bw)\n",
    "        loss_train = loss_function(pred_logits_bv, sentiment_b)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss_cumul += loss_train.item()\n",
    "        pb.set_postfix({\"avg loss\": training_loss_cumul / (i + 1), \"lr\": lr})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tiktoken\n",
    "# special_tokens\n",
    "gpt2_encoder = tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = gpt2_encoder.max_token_value + 1\n",
    "\n",
    "def to_tensor(sentence):\n",
    "    \n",
    "    sentence = gpt2_encoder.encode(sentence)\n",
    "    if len(sentence) < 70:\n",
    "        sentence = sentence + [PADDING_VALUE] * (70 - len(sentence))\n",
    "\n",
    "    \n",
    "    return torch.Tensor(sentence).unsqueeze(0).to(DEVICE).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phrases present in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=4/4 ---- Offers a guilt-free trip into feel-good territory.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Offers a guilt-free trip into feel-good territory.\")))\n",
    "classifications.argmax(), classifications\n",
    "\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Offers a guilt-free trip into feel-good territory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=1/4 ---- Offers absolutely nothing I hadn't already seen.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Offers absolutely nothing I hadn't already seen.\")))\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Offers absolutely nothing I hadn't already seen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phrases I just came up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=3/4 ---- Amazing stuff, love it.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Amazing stuff, love it.\")))\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Amazing stuff, love it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=1/4 ---- Pffft terrible, how could this be made.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Pffft terrible, how could this be made.\")))\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Pffft terrible, how could this be made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1),\n",
       " tensor([[0.2414, 0.4820, 0.2110, 0.0403, 0.0254]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(\n",
    "\n",
    "    torch.tensor(sentences[1]).unsqueeze(0)\n",
    "))\n",
    "\n",
    "\n",
    "classifications.argmax(), classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
