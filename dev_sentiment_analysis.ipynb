{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch import nn\n",
    "from model import ModelFactory\n",
    "from model.sequence_encoder import SequenceEncoder as SequenceEmbedder\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "DEVICE = \"cpu\" # \"cuda\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'dev'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"stanfordSentimentTreebank.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125, 70)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on simple Embedder + Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1): 100%|██████████| 41/41 [00:12<00:00,  3.23it/s, avg loss=0.329, lr=0.01, test loss=0.00815]\n",
      "(2): 100%|██████████| 41/41 [00:12<00:00,  3.20it/s, avg loss=0.184, lr=0.01, test loss=0.0081] \n",
      "(3): 100%|██████████| 41/41 [00:12<00:00,  3.22it/s, avg loss=0.0947, lr=0.01, test loss=0.0081] \n",
      "(4): 100%|██████████| 41/41 [00:12<00:00,  3.21it/s, avg loss=0.0463, lr=0.01, test loss=0.00809]\n",
      "(5):  49%|████▉     | 20/41 [00:06<00:06,  3.07it/s, avg loss=0.0277, lr=0.01, test loss=0.00833]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m pred_logits_bv \u001b[38;5;241m=\u001b[39m model(sentence_bw)\n\u001b[1;32m     95\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m loss_function(pred_logits_bv, sentiment_b) \u001b[38;5;241m+\u001b[39m l1_regularization(model, \u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mloss_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     99\u001b[0m training_loss_cumul \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_train\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_factory = ModelFactory(\n",
    "    coordinates = 200,\n",
    "    number_of_heads = 1, # ignroe this \n",
    "    words = 70,\n",
    "    tokens=50258 + 1,\n",
    ")\n",
    "\n",
    "p = 0.1\n",
    "\n",
    "class EmbedderPlusFeedForwar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedder = SequenceEmbedder(model_factory\n",
    "                                         )\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(model_factory.coordinates),\n",
    "           nn.Linear(model_factory.coordinates , model_factory.coordinates),\n",
    "           nn.Dropout(p),\n",
    "            nn.GELU(),\n",
    "           nn.Linear(model_factory.coordinates , model_factory.coordinates  // 2 ),\n",
    "           nn.Dropout(p),\n",
    "\n",
    "            nn.GELU(),\n",
    "           nn.Linear(model_factory.coordinates // 2  , model_factory.coordinates // 4 ),\n",
    "           nn.Dropout(p),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(model_factory.coordinates  // 4   , 1),\n",
    "        )\n",
    "\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.LayerNorm(model_factory.words),\n",
    "\n",
    "            nn.Linear(model_factory.words , model_factory.words),\n",
    "           nn.Dropout(p),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(model_factory.words, 5),\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence_bw: Tensor) -> Tensor:\n",
    "        sequence_bwc = self.embedder(sequence_bw)\n",
    "        sequence_bw1 = self.projection(sequence_bwc)\n",
    "        sequence_bw = sequence_bw1.squeeze(-1)\n",
    "        sequence_bv = self.classification_head(sequence_bw)\n",
    "        return sequence_bv\n",
    "\n",
    "\n",
    "gpt2_encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "model = EmbedderPlusFeedForwar().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss() # weight=torch.tensor([1.1, 1, 0.95, 1, 1.1], device=DEVICE)\n",
    "batch_size = 200\n",
    "sentences, sentiments = dataset['train']\n",
    "size = len(sentences)\n",
    "warmup_steps = 8\n",
    "get_lr = lambda step: min(step ** -0.5, step * warmup_steps ** -1.5) * model_factory.coordinates ** -0.5\n",
    "EPOCHS = 150\n",
    "test_sentences, test_sentiments = dataset['test']\n",
    "\n",
    "def l1_regularization(model, lambda_l1):\n",
    "    l1_penalty = 0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.abs(param).sum()\n",
    "    return lambda_l1 * l1_penalty\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    test_loss_cumul = 0\n",
    "    training_loss_cumul = 0\n",
    "    pb = tqdm(\n",
    "          range(0, size, batch_size),\n",
    "          desc=f\"({epoch})\",\n",
    "          leave=True,\n",
    "    )\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    indices = np.arange(len(sentences))\n",
    "    np.random.shuffle(indices)\n",
    "    # Use the shuffled indices to randomize sentences and sentiments\n",
    "    sentences = sentences[indices]\n",
    "    sentiments = sentiments[indices]\n",
    "    lr = get_lr(epoch)\n",
    "    lr = 1e-2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    model.train()\n",
    "    for i in pb:\n",
    "        sentence_bw = torch.tensor(sentences[i:i+batch_size]).to(DEVICE)\n",
    "        sentiment_b = torch.tensor(sentiments[i:i+batch_size]).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred_logits_bv = model(sentence_bw)\n",
    "        loss_train = loss_function(pred_logits_bv, sentiment_b) + l1_regularization(model, 0.00001)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss_cumul += loss_train.item()\n",
    "        # pb.set_postfix({\"avg loss\": training_loss_cumul / (i + 1), \"lr\": lr})\n",
    "        \n",
    "        tes_size = len(test_sentences)\n",
    "        random_index = np.random.randint(0, tes_size - batch_size)\n",
    "        sentence_bw = torch.tensor(test_sentences[random_index:random_index+batch_size]).to(DEVICE)\n",
    "        sentiment_b = torch.tensor(test_sentiments[random_index:random_index+batch_size]).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            correct = 0\n",
    "            pred_logits_bv = model(sentence_bw)\n",
    "            test_loss_cumul += loss_function(pred_logits_bv, sentiment_b).item()\n",
    "            pb.set_postfix({\n",
    "                \"avg loss\": training_loss_cumul / (i + 1),\n",
    "                \"lr\": lr,\n",
    "                \"test loss\": test_loss_cumul / (i + 1),\n",
    "            }\n",
    "            )\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PADDING_VALUE = gpt2_encoder.max_token_value + 1\n",
    "\n",
    "def to_tensor(sentence):\n",
    "    \n",
    "    sentence = gpt2_encoder.encode(sentence)\n",
    "    if len(sentence) < 70:\n",
    "        sentence = sentence + [PADDING_VALUE] * (70 - len(sentence))\n",
    "\n",
    "    \n",
    "    return torch.Tensor(sentence).unsqueeze(0).to(DEVICE).long()\n",
    "\n",
    "def classify(text):\n",
    "\n",
    "    classifications = nn.Softmax(dim=1)(model(to_tensor(text)))\n",
    "    return classifications.argmax().item() / 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phrase is present in the dataset with about 0.8 classification (which would indeed round to 1 due to labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however, the model does not understand when I modify it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"very bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_factory = ModelFactory(\n",
    "    coordinates = 9,\n",
    "    words = 70,\n",
    "    tokens=50258 + 1,\n",
    "    number_of_blocks = 1,\n",
    "    number_of_heads = 3,\n",
    "    bias = 0,\n",
    "    attention = \"metric\"# \"scaled_dot_product\", # or \"metric\"\n",
    ")\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model_factory.create_model(kind=\"encoder\")\n",
    "        del self.model[-1]\n",
    "        del self.model[-1]\n",
    "\n",
    " \n",
    "        self.projection = nn.Sequential(\n",
    "        nn.LayerNorm(model_factory.coordinates),\n",
    "          nn.Linear(model_factory.coordinates, 1),\n",
    "        )\n",
    "          \n",
    "      \n",
    "        self.classification_head = nn.Linear(model_factory.words, 5)\n",
    "\n",
    "    def forward(self, sequence_bw: Tensor) -> Tensor:\n",
    "        sequence_bwc = self.model(sequence_bw)\n",
    "        sequence_bw1 = self.projection(sequence_bwc)\n",
    "        sequence_bw = sequence_bw1.squeeze(-1)\n",
    "        sequence_bv = self.classification_head(sequence_bw)\n",
    "        return sequence_bv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ScaledDotProductAttention.__init__() got an unexpected keyword argument 'is_causal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[229], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m gpt2_encoder \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     15\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1.1\u001b[39m], device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "Cell \u001b[0;32mIn[60], line 14\u001b[0m, in \u001b[0;36mSentimentModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/workspaces/llm-voice-chat/model/__main__.py:83\u001b[0m, in \u001b[0;36mModelFactory.create_model\u001b[0;34m(self, kind)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_branch(kind)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_branch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder-decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EncoderDecoder(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_branch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/workspaces/llm-voice-chat/model/__main__.py:93\u001b[0m, in \u001b[0;36mModelFactory._create_branch\u001b[0;34m(self, kind)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_branch\u001b[39m(\u001b[38;5;28mself\u001b[39m, kind: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     90\u001b[0m     block \u001b[38;5;241m=\u001b[39m TransformerEncoderBlock \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m TransformerDecoderBlock\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     92\u001b[0m         SequenceEncoder(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m---> 93\u001b[0m         \u001b[38;5;241m*\u001b[39m[block(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_blocks)],\n\u001b[1;32m     94\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates),\n\u001b[1;32m     95\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias),\n\u001b[1;32m     96\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m/workspaces/llm-voice-chat/model/__main__.py:93\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_branch\u001b[39m(\u001b[38;5;28mself\u001b[39m, kind: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     90\u001b[0m     block \u001b[38;5;241m=\u001b[39m TransformerEncoderBlock \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m TransformerDecoderBlock\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     92\u001b[0m         SequenceEncoder(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m---> 93\u001b[0m         \u001b[38;5;241m*\u001b[39m[\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_blocks)],\n\u001b[1;32m     94\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates),\n\u001b[1;32m     95\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinates, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias),\n\u001b[1;32m     96\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m/workspaces/llm-voice-chat/model/transformer_block.py:78\u001b[0m, in \u001b[0;36mTransformerEncoderBlock.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params: TransformerBlockParameters):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTransformerEncoderBlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/llm-voice-chat/model/transformer_block.py:62\u001b[0m, in \u001b[0;36m_BaseTransformerBlock.__init__\u001b[0;34m(self, params, is_causal)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer \u001b[38;5;241m=\u001b[39m MetricSelfAttention(params, is_causal \u001b[38;5;241m=\u001b[39m is_causal)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_dot_product\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer \u001b[38;5;241m=\u001b[39m \u001b[43mScaledDotProductAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid attention type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ScaledDotProductAttention.__init__() got an unexpected keyword argument 'is_causal'"
     ]
    }
   ],
   "source": [
    "print(\"here\")\n",
    "DEVICE = \"cpu\" # \"cuda\"\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "gpt2_encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SentimentModel().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "class_weights = torch.tensor([1.1, 1, 0.95, 1, 1.1], device=DEVICE)\n",
    "loss_function = nn.CrossEntropyLoss() # weight=class_weights\n",
    "batch_size = 200\n",
    "sentences, sentiments = dataset['train']\n",
    "size = len(sentences)\n",
    "\n",
    "warmup_steps = 8\n",
    "get_lr = lambda step: min(step ** -0.5, step * warmup_steps ** -1.5) * model_factory.coordinates ** -0.5\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    \n",
    "    training_loss_cumul = 0\n",
    "    pb = tqdm(\n",
    "          range(0, size, batch_size),\n",
    "          desc=f\"({epoch})\",\n",
    "          leave=True,\n",
    "    )\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    indices = np.arange(len(sentences))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Use the shuffled indices to randomize sentences and sentiments\n",
    "    sentences = sentences[indices]\n",
    "    sentiments = sentiments[indices]\n",
    "    lr = get_lr(epoch)\n",
    "    for i in pb:\n",
    "        \n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        sentence_bw = torch.tensor(sentences[i:i+batch_size]).to(DEVICE)\n",
    "        sentiment_b = torch.tensor(sentiments[i:i+batch_size]).to(DEVICE)\n",
    "  \n",
    "        optimizer.zero_grad()\n",
    "        pred_logits_bv = model(sentence_bw)\n",
    "        loss_train = loss_function(pred_logits_bv, sentiment_b)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss_cumul += loss_train.item()\n",
    "        # pb.set_postfix({\"avg loss\": training_loss_cumul / (i + 1), \"lr\": lr})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_sentences, test_sentiments = dataset['test']\n",
    "            size = len(test_sentences)\n",
    "            test_loss_cumul = 0\n",
    "            correct = 0\n",
    "\n",
    "            random_index = np.random.randint(0, size - batch_size)\n",
    "            sentence_bw = torch.tensor(test_sentences[random_index:random_index+batch_size]).to(DEVICE)\n",
    "            sentiment_b = torch.tensor(test_sentiments[random_index:random_index+batch_size]).to(DEVICE)\n",
    "            pred_logits_bv = model(sentence_bw)\n",
    "            correct += (pred_logits_bv.argmax(dim=-1) == sentiment_b).sum().item()\n",
    "\n",
    "            accuracy = correct / size\n",
    "            pb.set_postfix({\n",
    "                \"avg loss\": training_loss_cumul / (i + 1),\n",
    "                \"lr\": lr,\n",
    "                \"test loss\": loss_function(pred_logits_bv, sentiment_b).item(),\n",
    "                \"accuracy\": accuracy\n",
    "            }\n",
    "            )\n",
    "            model.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PADDING_VALUE = gpt2_encoder.max_token_value + 1\n",
    "\n",
    "def to_tensor(sentence):\n",
    "    \n",
    "    sentence = gpt2_encoder.encode(sentence)\n",
    "    if len(sentence) < 70:\n",
    "        sentence = sentence + [PADDING_VALUE] * (70 - len(sentence))\n",
    "\n",
    "    \n",
    "    return torch.Tensor(sentence).unsqueeze(0).to(DEVICE).long()\n",
    "\n",
    "def classify(text):\n",
    "\n",
    "    classifications = nn.Softmax(dim=1)(model(to_tensor(text)))\n",
    "    return classifications.argmax().item() / 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"works, it's thanks to Huston's revelatory performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"amazing movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = gpt2_encoder.max_token_value + 1\n",
    "\n",
    "def to_tensor(sentence):\n",
    "    \n",
    "    sentence = gpt2_encoder.encode(sentence)\n",
    "    if len(sentence) < 70:\n",
    "        sentence = sentence + [PADDING_VALUE] * (70 - len(sentence))\n",
    "\n",
    "    \n",
    "    return torch.Tensor(sentence).unsqueeze(0).to(DEVICE).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phrases present in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=4/4 ---- Offers a guilt-free trip into feel-good territory.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Offers a guilt-free trip into feel-good territory.\")))\n",
    "classifications.argmax(), classifications\n",
    "\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Offers a guilt-free trip into feel-good territory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=1/4 ---- Offers absolutely nothing I hadn't already seen.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Offers absolutely nothing I hadn't already seen.\")))\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Offers absolutely nothing I hadn't already seen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phrases I just came up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=3/4 ---- Amazing stuff, love it.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Amazing stuff, love it.\")))\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Amazing stuff, love it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED_RATING=1/4 ---- Pffft terrible, how could this be made.\n"
     ]
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(to_tensor(\"Pffft terrible, how could this be made.\")))\n",
    "pred = classifications.argmax().item()\n",
    "\n",
    "print(f\"PREDICTED_RATING={pred}/4 ---- Pffft terrible, how could this be made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1),\n",
       " tensor([[0.2414, 0.4820, 0.2110, 0.0403, 0.0254]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = nn.Softmax(dim=1)(model(\n",
    "\n",
    "    torch.tensor(sentences[1]).unsqueeze(0)\n",
    "))\n",
    "\n",
    "\n",
    "classifications.argmax(), classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1):   0%|          | 0/33 [00:00<?, ?it/s, avg loss=1.69, lr=0.001, test loss=1.64]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1): 100%|██████████| 33/33 [00:01<00:00, 28.96it/s, avg loss=0.00656, lr=0.001, test loss=0.00657]\n",
      "(2): 100%|██████████| 33/33 [00:01<00:00, 28.05it/s, avg loss=0.00647, lr=0.001, test loss=0.00652]\n",
      "(3): 100%|██████████| 33/33 [00:01<00:00, 29.23it/s, avg loss=0.00645, lr=0.001, test loss=0.00651]\n",
      "(4): 100%|██████████| 33/33 [00:01<00:00, 30.63it/s, avg loss=0.00643, lr=0.001, test loss=0.00651]\n",
      "(5): 100%|██████████| 33/33 [00:01<00:00, 27.68it/s, avg loss=0.00641, lr=0.001, test loss=0.00651]\n",
      "(6): 100%|██████████| 33/33 [00:01<00:00, 29.13it/s, avg loss=0.00639, lr=0.001, test loss=0.00652]\n",
      "(7): 100%|██████████| 33/33 [00:01<00:00, 30.60it/s, avg loss=0.00635, lr=0.001, test loss=0.00651]\n",
      "(8): 100%|██████████| 33/33 [00:01<00:00, 29.24it/s, avg loss=0.0063, lr=0.001, test loss=0.00652] \n",
      "(9): 100%|██████████| 33/33 [00:01<00:00, 30.89it/s, avg loss=0.00622, lr=0.001, test loss=0.00653]\n",
      "(10): 100%|██████████| 33/33 [00:00<00:00, 62.85it/s, avg loss=0.00614, lr=0.001, test loss=0.00654]\n",
      "(11): 100%|██████████| 33/33 [00:00<00:00, 61.97it/s, avg loss=0.0061, lr=0.001, test loss=0.00656] \n",
      "(12): 100%|██████████| 33/33 [00:00<00:00, 57.36it/s, avg loss=0.00606, lr=0.001, test loss=0.00658]\n",
      "(13): 100%|██████████| 33/33 [00:00<00:00, 56.03it/s, avg loss=0.00604, lr=0.001, test loss=0.00661]\n",
      "(14): 100%|██████████| 33/33 [00:00<00:00, 61.23it/s, avg loss=0.00601, lr=0.001, test loss=0.00662]\n",
      "(15): 100%|██████████| 33/33 [00:00<00:00, 61.57it/s, avg loss=0.006, lr=0.001, test loss=0.00665]  \n",
      "(16): 100%|██████████| 33/33 [00:00<00:00, 58.81it/s, avg loss=0.00599, lr=0.001, test loss=0.00668]\n",
      "(17): 100%|██████████| 33/33 [00:00<00:00, 62.14it/s, avg loss=0.00599, lr=0.001, test loss=0.00668]\n",
      "(18): 100%|██████████| 33/33 [00:00<00:00, 60.60it/s, avg loss=0.00598, lr=0.001, test loss=0.00669]\n",
      "(19): 100%|██████████| 33/33 [00:00<00:00, 62.03it/s, avg loss=0.00597, lr=0.001, test loss=0.00672]\n",
      "(20): 100%|██████████| 33/33 [00:00<00:00, 60.96it/s, avg loss=0.00596, lr=0.001, test loss=0.00672]\n",
      "(21): 100%|██████████| 33/33 [00:00<00:00, 61.13it/s, avg loss=0.00596, lr=0.001, test loss=0.00673]\n",
      "(22): 100%|██████████| 33/33 [00:00<00:00, 60.89it/s, avg loss=0.00595, lr=0.001, test loss=0.00673]\n",
      "(23): 100%|██████████| 33/33 [00:00<00:00, 60.07it/s, avg loss=0.00597, lr=0.001, test loss=0.00675]\n",
      "(24): 100%|██████████| 33/33 [00:00<00:00, 53.56it/s, avg loss=0.00596, lr=0.001, test loss=0.00674]\n",
      "(25): 100%|██████████| 33/33 [00:00<00:00, 60.72it/s, avg loss=0.00596, lr=0.001, test loss=0.00676]\n",
      "(26): 100%|██████████| 33/33 [00:00<00:00, 60.78it/s, avg loss=0.00595, lr=0.001, test loss=0.00675]\n",
      "(27): 100%|██████████| 33/33 [00:00<00:00, 60.42it/s, avg loss=0.00596, lr=0.001, test loss=0.00677]\n",
      "(28): 100%|██████████| 33/33 [00:00<00:00, 53.97it/s, avg loss=0.00596, lr=0.001, test loss=0.00676]\n",
      "(29): 100%|██████████| 33/33 [00:00<00:00, 61.71it/s, avg loss=0.00596, lr=0.001, test loss=0.00677]\n",
      "(30): 100%|██████████| 33/33 [00:00<00:00, 61.83it/s, avg loss=0.00595, lr=0.001, test loss=0.00676]\n",
      "(31): 100%|██████████| 33/33 [00:00<00:00, 56.75it/s, avg loss=0.00595, lr=0.001, test loss=0.00678]\n",
      "(32): 100%|██████████| 33/33 [00:00<00:00, 61.42it/s, avg loss=0.00595, lr=0.001, test loss=0.00677]\n",
      "(33): 100%|██████████| 33/33 [00:00<00:00, 61.57it/s, avg loss=0.00595, lr=0.001, test loss=0.00676]\n",
      "(34): 100%|██████████| 33/33 [00:00<00:00, 60.79it/s, avg loss=0.00595, lr=0.001, test loss=0.00677]\n",
      "(35): 100%|██████████| 33/33 [00:00<00:00, 53.66it/s, avg loss=0.00594, lr=0.001, test loss=0.00678]\n",
      "(36): 100%|██████████| 33/33 [00:00<00:00, 61.84it/s, avg loss=0.00594, lr=0.001, test loss=0.00677]\n",
      "(37): 100%|██████████| 33/33 [00:00<00:00, 61.84it/s, avg loss=0.00594, lr=0.001, test loss=0.00678]\n",
      "(38): 100%|██████████| 33/33 [00:00<00:00, 61.42it/s, avg loss=0.00594, lr=0.001, test loss=0.00677]\n",
      "(39): 100%|██████████| 33/33 [00:00<00:00, 60.72it/s, avg loss=0.00595, lr=0.001, test loss=0.00679]\n",
      "(40): 100%|██████████| 33/33 [00:00<00:00, 61.50it/s, avg loss=0.00594, lr=0.001, test loss=0.00677]\n",
      "(41): 100%|██████████| 33/33 [00:00<00:00, 61.45it/s, avg loss=0.00595, lr=0.001, test loss=0.00678]\n",
      "(42): 100%|██████████| 33/33 [00:00<00:00, 60.64it/s, avg loss=0.00595, lr=0.001, test loss=0.00679]\n",
      "(43): 100%|██████████| 33/33 [00:00<00:00, 60.96it/s, avg loss=0.00594, lr=0.001, test loss=0.00677]\n",
      "(44): 100%|██████████| 33/33 [00:00<00:00, 61.44it/s, avg loss=0.00594, lr=0.001, test loss=0.00677]\n",
      "(45): 100%|██████████| 33/33 [00:00<00:00, 61.70it/s, avg loss=0.00594, lr=0.001, test loss=0.00678]\n",
      "(46):  55%|█████▍    | 18/33 [00:00<00:00, 52.93it/s, avg loss=0.00611, lr=0.001, test loss=0.00694]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    126\u001b[0m     correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 127\u001b[0m     pred_logits_bv \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_bw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     test_loss_cumul \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_function(pred_logits_bv, sentiment_b)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    129\u001b[0m     pb\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: training_loss_cumul \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: lr,\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_loss_cumul \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    133\u001b[0m     })\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 35\u001b[0m, in \u001b[0;36mSentimentModel.forward\u001b[0;34m(self, sequence_bw)\u001b[0m\n\u001b[1;32m     33\u001b[0m sequence_bwc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sequence_bw)\n\u001b[1;32m     34\u001b[0m sequence_bwc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(sequence_bwc)\n\u001b[0;32m---> 35\u001b[0m sequence_bw1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_bwc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m sequence_bw \u001b[38;5;241m=\u001b[39m sequence_bw1\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m sequence_bv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head(sequence_bw)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_factory = ModelFactory(\n",
    "    coordinates = 50,\n",
    "    words = 70,\n",
    "    tokens=50258 + 1,\n",
    "    number_of_blocks = 2,\n",
    "    number_of_heads = 25,\n",
    "    bias = 0,\n",
    "    attention = \"metric\"# \"scaled_dot_product\", # or \"metric\"\n",
    ")\n",
    "\n",
    "\n",
    "p = 0\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model_factory.create_model(kind=\"encoder\")[0]\n",
    " \n",
    "\n",
    " \n",
    "        self.layer_norm = nn.LayerNorm(model_factory.coordinates)\n",
    "        self.projection = nn.Sequential(\n",
    "        nn.LayerNorm(model_factory.coordinates),\n",
    "          nn.Linear(model_factory.coordinates, 1),\n",
    "          nn.Dropout(p),\n",
    "        )\n",
    "          \n",
    "      \n",
    "        self.classification_head = nn.Sequential(\n",
    "          nn.Linear(model_factory.words, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence_bw: Tensor) -> Tensor:\n",
    "        sequence_bwc = self.model(sequence_bw)\n",
    "        sequence_bwc = self.layer_norm(sequence_bwc)\n",
    "        sequence_bw1 = self.projection(sequence_bwc)\n",
    "        sequence_bw = sequence_bw1.squeeze(-1)\n",
    "        sequence_bv = self.classification_head(sequence_bw)\n",
    "        return sequence_bv\n",
    "\n",
    "\n",
    "print(\"here\")\n",
    "DEVICE = \"cpu\" # \"cuda\"\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "gpt2_encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "test_sentences, test_sentiments = dataset['test']\n",
    "\n",
    "\n",
    "model = SentimentModel().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "class_weights = torch.tensor([1.1, 1, 0.95, 1, 1.1], device=DEVICE)\n",
    "loss_function = nn.CrossEntropyLoss() # weight=class_weights\n",
    "batch_size = 250\n",
    "sentences, sentiments = dataset['train']\n",
    "size = len(sentences)\n",
    "\n",
    "warmup_steps = 8\n",
    "get_lr = lambda step: min(step ** -0.5, step * warmup_steps ** -1.5) * model_factory.coordinates ** -0.5\n",
    "EPOCHS = 100\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def l1_regularization(model, lambda_l1):\n",
    "    l1_penalty = 0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.abs(param).sum()\n",
    "    return lambda_l1 * l1_penalty\n",
    "\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    if epoch == 10:\n",
    "        for param in model.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    test_loss_cumul = 0\n",
    "    training_loss_cumul = 0\n",
    "    pb = tqdm(\n",
    "          range(0, size, batch_size),\n",
    "          desc=f\"({epoch})\",\n",
    "          leave=True,\n",
    "    )\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    indices = np.arange(len(sentences))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Use the shuffled indices to randomize sentences and sentiments\n",
    "    sentences = sentences[indices]\n",
    "    sentiments = sentiments[indices]\n",
    "\n",
    "    for i in pb:\n",
    "        lr = 1e-3\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        sentence_bw = torch.tensor(sentences[i:i+batch_size]).to(DEVICE)\n",
    "        sentiment_b = torch.tensor(sentiments[i:i+batch_size]).to(DEVICE)\n",
    "  \n",
    "        optimizer.zero_grad()\n",
    "        pred_logits_bv = model(sentence_bw)\n",
    "        loss_train = loss_function(pred_logits_bv, sentiment_b) # + l1_regularization(model, 0.0001)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss_cumul += loss_train.item()\n",
    "        # pb.set_postfix({\"avg loss\": training_loss_cumul / (i + 1), \"lr\": lr})\n",
    "\n",
    "\n",
    "        tes_size = len(test_sentences)\n",
    "        random_index = i if i < tes_size - batch_size else tes_size - batch_size - 1\n",
    "        sentence_bw = torch.tensor(test_sentences[random_index:random_index+batch_size]).to(DEVICE)\n",
    "        sentiment_b = torch.tensor(test_sentiments[random_index:random_index+batch_size]).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            correct = 0\n",
    "            pred_logits_bv = model(sentence_bw)\n",
    "            test_loss_cumul += loss_function(pred_logits_bv, sentiment_b).item()\n",
    "            pb.set_postfix({\n",
    "                \"avg loss\": training_loss_cumul / (i + 1),\n",
    "                \"lr\": lr,\n",
    "                \"test loss\": test_loss_cumul / (i + 1),\n",
    "            })\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1):   0%|          | 0/508 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1):  85%|████████▌ | 433/508 [15:32<02:37,  2.10s/it, avg loss=0.363, lr=0.001, test loss=0.359]"
     ]
    }
   ],
   "source": [
    "\n",
    "model_factory = ModelFactory(\n",
    "    coordinates = 9,\n",
    "    words = 70,\n",
    "    tokens=50258 + 1,\n",
    "    number_of_blocks = 2,\n",
    "    number_of_heads = 3,\n",
    "    bias = 0,\n",
    "    attention = \"metric\"# \"scaled_dot_product\", # or \"metric\"\n",
    ")\n",
    "\n",
    "\n",
    "model = model_factory.create_model(kind=\"encoder\")\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "gpt2_encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "loss_function = nn.CrossEntropyLoss() # weight=torch.tensor([1.1, 1, 0.95, 1, 1.1], device=DEVICE)\n",
    "batch_size = 16\n",
    "EPOCHS = 1\n",
    "\n",
    "test_sentences, test_sentiments = dataset['test']\n",
    "sentences, sentiments = dataset['train']\n",
    "\n",
    "sentences = torch.tensor(sentences).to(DEVICE)\n",
    "sentiments = torch.tensor(sentiments).to(DEVICE)\n",
    "test_sentences = torch.tensor(test_sentences).to(DEVICE)\n",
    "test_sentiments = torch.tensor(test_sentiments).to(DEVICE)\n",
    "size = len(sentences)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    test_loss_cumul = 0\n",
    "    training_loss_cumul = 0\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    indices = np.arange(len(sentences))\n",
    "    np.random.shuffle(indices)\n",
    "    sentences = sentences[indices]\n",
    "    sentiments = sentiments[indices]\n",
    "\n",
    "    for i in (pb := tqdm(range(0, size, batch_size), desc=f\"({epoch})\", leave=True)):\n",
    "     \n",
    "        sentence_bw = sentences[i:i+batch_size]\n",
    "        gt_sentence_bw = sentences[i:i+batch_size]\n",
    "        sentiment_b = sentiments[i:i+batch_size]\n",
    "        gt_sentence_bw[:, -1] = sentiment_b\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_logits_bwt = model(sentence_bw)\n",
    "        pred_logits_btw = pred_logits_bwt.transpose(-1, -2)\n",
    "        loss_train = loss_function(pred_logits_btw, gt_sentence_bw) # + l1_regularization(model, 0.0001)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss_cumul += loss_train.item()\n",
    "        tes_size = len(test_sentences)\n",
    "        random_index = i if i < tes_size - batch_size else tes_size - batch_size - 1\n",
    "        sentence_bw = test_sentences[random_index:random_index+batch_size]\n",
    "        gt_sentence_bw = test_sentences[random_index:random_index+batch_size]\n",
    "        sentiment_b = test_sentiments[random_index:random_index+batch_size]\n",
    "        gt_sentence_bw[:, -1] = sentiment_b\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            pred_logits_bwt = model(sentence_bw)\n",
    "            pred_logits_btw = pred_logits_bwt.transpose(-1, -2)\n",
    "            test_loss_cumul += loss_function(pred_logits_btw, gt_sentence_bw).item()\n",
    "            pb.set_postfix({\n",
    "                \"avg loss\": training_loss_cumul / (i + 1),\n",
    "                \"lr\": lr,\n",
    "                \"test loss\": test_loss_cumul / (i + 1),\n",
    "            })\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
